# How Vector Search Works in Your RAG System

## ğŸ¯ The Complete Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INDEXING (Done once when you run process_pdfs.py)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  PDF Document                                                   â”‚
â”‚      â†“                                                          â”‚
â”‚  Extract Text â†’ "Machine learning is..."                       â”‚
â”‚      â†“                                                          â”‚
â”‚  Split into Chunks â†’ ["Machine learning is...", "Neural..."]   â”‚
â”‚      â†“                                                          â”‚
â”‚  Generate Embeddings:                                           â”‚
â”‚      "Machine learning is..." â†’ [0.23, -0.45, 0.89, ...]      â”‚
â”‚      "Neural networks..." â†’ [0.21, -0.47, 0.85, ...]           â”‚
â”‚      â†“                                                          â”‚
â”‚  Store in ChromaDB:                                             â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚      â”‚ Chunk ID â”‚ Text            â”‚ Vector (384-dim) â”‚         â”‚
â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
â”‚      â”‚ chunk_1  â”‚ "Machine..."    â”‚ [0.23, -0.45...] â”‚         â”‚
â”‚      â”‚ chunk_2  â”‚ "Neural..."     â”‚ [0.21, -0.47...] â”‚         â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUERYING (Every time you ask a question)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Your Question: "What is ML?"                                   â”‚
â”‚      â†“                                                          â”‚
â”‚  [1] Convert to Vector                                          â”‚
â”‚      "What is ML?" â†’ [0.24, -0.44, 0.88, ...] (384 numbers)    â”‚
â”‚      â†“                                                          â”‚
â”‚  [2] ChromaDB compares with ALL stored vectors                  â”‚
â”‚      Query vector vs chunk_1 vector â†’ Similarity: 0.95         â”‚
â”‚      Query vector vs chunk_2 vector â†’ Similarity: 0.82         â”‚
â”‚      Query vector vs chunk_3 vector â†’ Similarity: 0.45         â”‚
â”‚      ... (compares with all 145 chunks in ~10ms)               â”‚
â”‚      â†“                                                          â”‚
â”‚  [3] Rank by similarity                                         â”‚
â”‚      1. chunk_1 (95%)                                           â”‚
â”‚      2. chunk_2 (82%)                                           â”‚
â”‚      3. chunk_5 (74%)                                           â”‚
â”‚      â†“                                                          â”‚
â”‚  [4] Filter by threshold (60%)                                  â”‚
â”‚      âœ“ chunk_1 (95%) - PASS                                    â”‚
â”‚      âœ“ chunk_2 (82%) - PASS                                    â”‚
â”‚      âœ“ chunk_5 (74%) - PASS                                    â”‚
â”‚      âœ— chunk_8 (45%) - FILTERED OUT                            â”‚
â”‚      â†“                                                          â”‚
â”‚  [5] Return top results                                         â”‚
â”‚      ["Machine learning is...", "Neural networks..."]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ The Math Behind It

### 1. **Text â†’ Numbers (Embeddings)**

The `all-MiniLM-L6-v2` model converts text to 384-dimensional vectors:

```python
# Your query
"What is machine learning?"
    â†“
[0.234, -0.456, 0.891, ..., 0.123]  # 384 numbers
```

### 2. **Similarity Calculation (Cosine Similarity)**

Measures the angle between two vectors:

```python
def cosine_similarity(vec1, vec2):
    dot_product = vec1 Â· vec2
    magnitude = ||vec1|| Ã— ||vec2||
    return dot_product / magnitude
```

**Results:**
- `1.0` = Identical meaning
- `0.8-0.9` = Very similar
- `0.5-0.7` = Somewhat related
- `0.0` = Unrelated
- `-1.0` = Opposite meaning

### 3. **Why It Works**

Words with similar meanings are placed **near each other** in vector space:

```
Vector Space (simplified to 2D):

    "neural networks" â—
                      â†‘ (close = similar)
    "machine learning" â—

    (far away)
         â†“
    "weather forecast" â—
```

## ğŸ”¬ Real Example

Let's trace a query through the system:

### Query: "How does AI work?"

**Step 1: Embed Query**
```
"How does AI work?" â†’ [0.45, -0.23, 0.67, ..., 0.89]
```

**Step 2: Documents in ChromaDB**
```
Doc A: "Artificial intelligence learns from data"
       Vector: [0.47, -0.25, 0.65, ..., 0.87]

Doc B: "Neural networks process information"
       Vector: [0.43, -0.21, 0.69, ..., 0.85]

Doc C: "The recipe calls for 2 cups of flour"
       Vector: [-0.12, 0.67, -0.34, ..., 0.23]
```

**Step 3: Calculate Similarities**
```
cosine(query, Doc A) = 0.94  â†’ 94% similar âœ…
cosine(query, Doc B) = 0.87  â†’ 87% similar âœ…
cosine(query, Doc C) = 0.12  â†’ 12% similar âŒ
```

**Step 4: Return Top Results**
```
1. Doc A (94%) - "Artificial intelligence learns from data"
2. Doc B (87%) - "Neural networks process information"
```

**Note:** Doc A is #1 even though it says "artificial intelligence" not "AI"!

## ğŸ¨ Visual Representation

Imagine each piece of text as a point in 384-dimensional space:

```
Similar meanings cluster together:

    â”Œâ”€ ML Cluster â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â— "machine learning"     â”‚
    â”‚  â— "neural networks"      â”‚
    â”‚  â— "deep learning"        â”‚
    â”‚  â— "AI algorithms"        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€ Programming Cluster â”€â”€â”€â”€â”
    â”‚  â— "Python code"          â”‚
    â”‚  â— "JavaScript"           â”‚
    â”‚  â— "programming"          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€ Weather Cluster â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â— "rain forecast"        â”‚
    â”‚  â— "temperature"          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

When you query "What is ML?", the system finds the
point closest to your query in this space.
```

## âš¡ What Happens in Your Code

### In `vectorize.py`:

```python
# When you call db.query()
results = self.collection.query(
    query_texts=[query_text],  # Your question
    n_results=n_results,       # How many results
    include=["documents", "metadatas", "distances"]
)
```

### ChromaDB does this internally:

1. **Embeds your query** using the same model that embedded the documents
2. **Computes cosine distance** to every stored vector (uses HNSW index for speed)
3. **Sorts by distance** (smaller distance = more similar)
4. **Returns top N** results with their distances

### Distance â†’ Similarity Conversion:

ChromaDB returns **distance** (0 = perfect match, 2 = opposite):

```python
# In your CLI (query_cli.py)
similarity = max(0, 100 * (1 - distance / 2))

# Examples:
distance = 0.0  â†’ similarity = 100%  (identical)
distance = 0.4  â†’ similarity = 80%   (very similar)
distance = 1.0  â†’ similarity = 50%   (somewhat similar)
distance = 2.0  â†’ similarity = 0%    (opposite)
```

## ğŸš€ Why This Is Fast

**Question:** How can it compare your query to 10,000 document chunks in milliseconds?

**Answer:** HNSW (Hierarchical Navigable Small World) Index

Instead of checking every vector:
```
âŒ Slow (Linear Search): Check all 10,000 vectors
âœ… Fast (HNSW Index): Check ~log(N) vectors

For 10,000 chunks:
  - Linear: 10,000 comparisons
  - HNSW: ~13 comparisons
```

It's like a tree structure that navigates to similar vectors quickly.

## ğŸ§ª Try It Yourself

Run the demo script to see this in action:

```bash
source venv/bin/activate
python explain_retrieval.py
```

This will show you:
- Actual embeddings for sample queries
- Similarity scores between your query and different documents
- Why some documents rank higher than others

## ğŸ’¡ Key Takeaways

1. **Everything becomes numbers** (384-dimensional vectors)
2. **Similar meanings = similar vectors** (close together in space)
3. **Cosine similarity** measures how similar vectors are
4. **No exact keyword needed** - it's all about semantic meaning
5. **HNSW index** makes it lightning fast even with millions of chunks

## ğŸ”— References

- **Sentence-Transformers**: https://www.sbert.net/
- **ChromaDB**: https://docs.trychroma.com/
- **HNSW Algorithm**: https://arxiv.org/abs/1603.09320
- **Cosine Similarity**: https://en.wikipedia.org/wiki/Cosine_similarity
